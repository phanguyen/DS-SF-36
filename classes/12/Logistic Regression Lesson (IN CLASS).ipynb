{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgemcintire/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "from IPython.display import Image\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.datasets import make_classification;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Logistic Regression</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression is a generalization of the linear regression model adapted to\n",
    " to classification problems.\n",
    " \n",
    "- Very popular because it's very fast and interpretable. Doesn't need scaling or much tuning.\n",
    "\n",
    "- Not vulnerable to overfitting when you don't have many features.\n",
    " \n",
    "- In linear regression, we use a set of quantitative feature variables to predict a continuous response variable. In logistic regression, we use a set of quantitative feature variables to predict probabilities of class membership.\n",
    "\n",
    "- Named for the function used at the core of the method, the logistic function aka the sigmoid function. \n",
    "\n",
    "- Logistic regression is a linear regression between our feature, X, and the log-odds of our data belonging to a certain class that we will call true for the sake of generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"logit_model.png\")\n",
    "#Source: Sinan Ozdemir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding graph represents the logistic function's ability to map our continuous input, x, to a smooth probability curve that begins at the left, near probability 0, and as we increase x, our probability of belonging to a certain class rises naturally and smoothly up to probability 1. \n",
    "\n",
    "\n",
    "In other words:\n",
    "\n",
    "    • Logistic regression gives an output of the probabilities of a specific class being true\n",
    "    \n",
    "    • Those probabilities can be converted into class predictions: if p>= 0.5 the models returns 1 and if p<.0.5 it returns 0\n",
    "    \n",
    "    • Logistic function is S-shaped and will always produced values > 0 and < 1.\n",
    "    \n",
    "    • Not all relationships as you know are linear, so LR is not always the right model.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Key difference in use of coefficients in linear vs logistic</b>\n",
    "<br>\n",
    "Linear Regression: Betas/coefficients represents the change in the response variable for a unit change in x. \n",
    "\n",
    "Logistic Regression: They represents the change in the log-odds. For a unit change in x. This means that e^β gives us the change in the odds for a unit change in x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use Sklearn to create fake data\n",
    "data = make_classification(n_samples=300,n_features=2,class_sep=.86,n_informative=2,\n",
    "                         n_redundant=0, n_repeated=0,n_classes=2, random_state=8)\n",
    "#returns a tuple with the features in index 0 and target in index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform fake data into pandas dataframe\n",
    "cols = [\"feature_one\", \"feature_two\"]\n",
    "df = pd.DataFrame()\n",
    "df[\"target\"] = \n",
    "#assign red to class 0 and blue to class 1. For plotting purposes.\n",
    "color = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the fake data and try to imagine what a linear boundary separating the two groups would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,11))\n",
    "plt.scatter(df.feature_one, df.feature_two, c=color, s=80)\n",
    "plt.xlabel(\"Feature One\",)\n",
    "plt.ylabel(\"Feature Two\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had to draw a straight line that best separates the two classes, where would you put the line?\n",
    "<br><br>\n",
    "Let's focus on Feature One and plot it against the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(df.feature_one, df.target, s=60)\n",
    "plt.xlabel(\"Feature One\")\n",
    "plt.ylabel(\"Target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats the relationship between the two variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the dataframe by the feature one and create a new data frame from that.\n",
    "df2 = df.sort(\"feature_one\").copy();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear regression model trained on feature one to predict the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create X and y variables\n",
    "X = df2[[\"feature_one\"]]\n",
    "y = df2.target\n",
    "#Intialize model\n",
    "\n",
    "#Fit model\n",
    "\n",
    "#Make predictions on X and assign them to pred variable\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plot the data with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel(\"Feature One\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.scatter(X,y, s=60)\n",
    "plt.plot(X, pred, c=\"r\", linewidth=3);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> How does this model look?</b>\n",
    "\n",
    "<b> Look at the S-function from above, how do you think it would fit the data? </b>\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "<b> Let's find out!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a logistic regression model on the data above and plot the predicted labels and the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Intialize and fit a logistic regression model\n",
    "logr = LogisticRegression();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score the model \n",
    "score = \n",
    "print \"The accuracy score is {:.2f} percent\".format(score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate label predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign label predictions to pred_labels\n",
    "pred_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate probability predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column indicates the predicted probability of class 0, and the second column indicates the predicted probability of class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign probability of class 1 to pred_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Put it all together and have a gander</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "plt.xlabel(\"Feature One\")\n",
    "plt.ylabel(\"Target\")\n",
    "\n",
    "plt.scatter(X,y, s=70, c= \"blue\", alpha=1, label=\"Scatter Plot Data\")\n",
    "plt.plot(X, pred, c=\"r\", linewidth=8, alpha=.5, label = \"Linear Regression Predictions\")\n",
    "plt.plot(X, pred_labels, c=\"y\", linewidth=8, alpha=.5, label = \"Logistic Regression Label Predictions\")\n",
    "plt.plot(X, pred_probs, c=\"g\", linewidth=8, alpha=.5, label = \"Logistic Regression Probability Predictions\")\n",
    "\n",
    "plt.legend(loc=4, fontsize=\"xx-large\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> What do you see? What catches your attention?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the original dataset with two features and visualize the linear boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function allows us to see the model\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y,s=40, alpha=0.4)\n",
    "    plt.xlabel(\"Feature One\")\n",
    "    plt.ylabel(\"Feature Two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create X and y variables from data using df\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df.target\n",
    "#Color code y\n",
    "color = y.map({0:\"blue\", 1:\"red\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Intialize model and fit it to X and y\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the model and the data into the plot_decision_boundary function\n",
    "# plot_decision_boundary(lr, X.values,color);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Does the boundary above match the one I asked you to imagine earlier? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Can you use Spotify data to predict whether or not I will like a song? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Attributes </b>\n",
    "\n",
    "\n",
    "    Acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
    "    \n",
    "    Danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
    "\n",
    "    Instrumentalness: Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
    "    \n",
    "    Loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n",
    "    \n",
    "    Mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
    "\n",
    "    Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
    "    \n",
    "    Tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
    "\n",
    "    Energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
    "    \n",
    "More information here https://developer.spotify.com/web-api/get-audio-features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset of songs I like and dislike. Target == 1 means liked song and Target == 0 means disliked song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the datafile \"Spotify_Data.pkl\" and check it out\n",
    "df = pd.read_pickle(\"Spotify_Data.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Quick EDA</b>: Summary stats grouped by class and correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What is the average value for each feature grouped by class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What are the correlations of the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Thoughts? Things of interest? Which variables stick out to you? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Train a logistic regression model on the data to predict whether or not I will like a certain song </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and y variables\n",
    "X = \n",
    "y = \n",
    "\n",
    "#Intialize, fit, and score the model\n",
    "lr = \n",
    "\n",
    "lr.fit()\n",
    "\n",
    "score = lr.score()\n",
    "\n",
    "print (\"The model produces an accuracy score of {:.2f} percent\".format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that a good or bad score? To find out let's compare it to the null accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the null accuracy aka the benchmark score. Percents for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Training/testing  </b>\n",
    "1. Split the data into train/test splits\n",
    "2. Fit data onto training set\n",
    "3. Make predictions on test set with the training model\n",
    "4. Calculate accuracy score by comparing predicted labels of the test set to its actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1. Use test_size = 0.4 and random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "\n",
    "#Step 2\n",
    "\n",
    "\n",
    "#Step 3\n",
    "\n",
    "\n",
    "#Step 4\n",
    "testing_score = accuracy_score()\n",
    "\n",
    "print (\"The model accurately classified {:.2f} percent of the testing data\".format(testing_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the testing accuracy compare to the first one?\n",
    "<br><br><br><br>\n",
    "Use cross validation to derive a truer testing accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use cross_val_score method to generate the average accuracy score for 5 CVs\n",
    "mean_cv_score = cross_val_score()\n",
    "\n",
    "print (\"The cross validated accuracy score is {:.2f} percent\").format(mean_cv_score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Probability, odds, e, log, log-odds</b>\n",
    "<br>\n",
    "Quick stats and probability detour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"odds_probs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table of probability versus odds\n",
    "table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})\n",
    "table['odds'] = table.probability/(1 - table.probability)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is e? It is the base rate of growth shared by all continually growing processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential function: e^1\n",
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a (natural) log? It gives you the time needed to reach a certain level of growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time needed to grow 1 unit to 2.718 units\n",
    "np.log(2.718)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also the inverse of the exponential function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(np.exp(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add log-odds to the table\n",
    "table['logodds'] = np.log(table.odds)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log odds are what is passed throught the logistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Train model using one feature: \"valence\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#V is the variable assigned to valence\n",
    "\n",
    "V = \n",
    "lr_V = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predicted log-odds for valence_value=0.5...\n",
    "#by multiplying it by coefficien and then adding the intercept to it\n",
    "\n",
    "valence_value = 0.5\n",
    "logodds = lr_V.intercept_ + lr_V.coef_[0]*valence_value\n",
    "logodds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert log-odds to odds\n",
    "odds = np.exp(logodds)\n",
    "odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert odds to probability\n",
    "prob = odds/(1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predicted probability for valence_value using the predict_proba method\n",
    "lr_V.predict_proba(valence_value)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n",
    "<br><br><br><br><br><br><br><br>\n",
    "Re-fit model but all the variables and make table of the coefficients and odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"target\", axis=1)\n",
    "y = df.target\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table of coefficients and their values\n",
    "coef = pd.DataFrame(zip(X.columns, np.transpose(lr.coef_[0])), columns=[\"coef\", \"value\"])\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odds ratio is the ratio of the odds(after increasing X_i by 1) over (divided) by odds(before increasing X_i by 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_odds = np.e**(coef[\"value\"])\n",
    "coef[\"odds_ratio\"] = coef_odds\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increase in probability is hard to quantify.  The lower p(before) is, the greater increase you'll have vs. a higher p(before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize how the Beta and intercept can affect the probabilities\n",
    "Image(\"curves.png\")\n",
    "#Source: Kevin Markham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the $\\beta_0$ or intercept value shifts the curve horizontally, whereas changing the $\\beta_1$ or coefficient value changes the slope of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> AUC and ROC Curves </b>\n",
    "\n",
    "ROC = Receiver operating characteristic\n",
    "<br>\n",
    "AUC = Area under curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Recall and Precision scores</b>\n",
    "<br><br>\n",
    "<b>Recall aka the True Positive Rate:</b> What percent of the data that is labelled class A was correctly identified as class A \n",
    "<br><br>\n",
    "<b>Precision:</b> What percent of the data that is classified as class A actually belongs to class A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"acc_rec_prec.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize example confusion matrix\n",
    "Image(\"con_matrix.png\")\n",
    "#Source: Kevin Markham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Create confusion matrix for the Spotify data and calculate recall and precision scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a train test split of the spotify data and train a logistic regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=4)\n",
    "\n",
    "preds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null accuracy of y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the predictions and y_test into a confusion matrix\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy, precision and recall scores\n",
    "acs = float(accuracy_score(,))\n",
    "ps = float(precision_score(,))\n",
    "rs = float(recall_score(,))\n",
    "\n",
    "print (\"Accuracy score = {:.2f}\".format(acs*100))\n",
    "print (\"Precision score = {:.2f}\".format(ps*100))\n",
    "print (\"Recall score = {:.2f}\".format(rs*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>False Positive Score:</b> Number of events wrongly classified as True divided by the number of negative events:\n",
    "<br><br>\n",
    "Example: Giving a bank loan to someone who defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have to calculate false positive score using our function\n",
    "\n",
    "def FPR(YT, PR): \n",
    "    \"\"\"\n",
    "    YT is the target variable for the testing set\n",
    "    PR is the predictions made on the features of the testing set\n",
    "    \"\"\"\n",
    "    CM = confusion_matrix(YT, PR)\n",
    "    #Fill in blank\n",
    "    return FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call FPR on the y_test and predicted labels objects\n",
    "FPR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onto plotting the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Derive probabilities of class 1 from the test set\n",
    "test_probs = lr.predict_proba(X_test)[:,1]\n",
    "#Pass in the test_probs variable and the true test labels aka y_test in the roc_curve function\n",
    "fpr, tpr, thres = roc_curve(y_test, test_probs)\n",
    "#Will explain the fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting False Positive Rates vs the True Positive Rates\n",
    "#Dotted line represents a useless model\n",
    "plt.figure(figsize=(15,11))\n",
    "plt.plot(fpr, tpr, linewidth=8)\n",
    "plt.plot([0,1], [0,1], \"--\", alpha=.7)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> What does this tell us about the model? Is it a good or bad model?<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caculate the area under the curve score using roc_auc_score\n",
    "roc_auc_score(y_test, test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some other roc curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"roc_curves.png\",width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> How thresholds affect the TPR and FPR? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(thres, fpr, linewidth=5)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"False Positive Rate\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(thres, tpr, linewidth=5)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"True Positive Rate\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see here? Why are is there a negative correlation in both graphs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Thresholds and model performance: Does tweaking the threshold give us a better model?</b>\n",
    "<br><br>\n",
    "Unfortunately there's no threshold to configure in a logistic regression model. \n",
    "<br>For example: lr = logisticregression(threshold=n) \n",
    "<br>\n",
    "<br>So we need to create our own threshold function using the np.where method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign all the values in test_probs >=0.7 == 1 and the rest equal to 0\n",
    "#First argument is condition\n",
    "#Second argument is the value you use to replace all the values that satisfy the condition\n",
    "#Third argument is the value you use to replace all the values that don't satisfy the condition\n",
    "labels_70 = np.where(test_probs>=0.7, 1, 0)\n",
    "labels_70[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this give a better accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy_score(y_test, labels_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put this in function form\n",
    "\n",
    "def thres_acc(T, YT, TP):\n",
    "    \"\"\"\n",
    "    T == Threshold input of our choosing\n",
    "    YT == The target variable for the testing set\n",
    "    TP == Probabilities of Class 1 for the features of the testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Plot various thresholds vs their accuracy scores</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an array of 20 values evenly spaced between 0 and 1\n",
    "thresholds = np.linspace(0,1, 30).tolist()\n",
    "acc_scores = [thres_acc(i, y_test, test_probs) for i in thresholds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot thresholds vs accuracy scores\n",
    "plt.figure(figsize=(15,11))\n",
    "plt.plot(thresholds, acc_scores, linewidth=5)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Accuracy Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which threshold produces the best accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dictornary of thresholds and acc_scores.\n",
    "thres_score_dict = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sort dictionary to find key with the highest value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>In class work:</b>\n",
    "\n",
    "    - Use feature engineering to create the best possible model to predict songs I like.\n",
    "    - Use ROC curve to evaluate your models.\n",
    "    - See which features are the most useful in predicting song category\n",
    "    - Experiment with .predict_proba(). Try to find the which values of the features produces the highest probability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
